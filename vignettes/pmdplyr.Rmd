---
title: "pmdplyr: Panel Maneuvers in dplyr"
author: "Nick Huntington-Klein, Philip Khor"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
<!-- output: rmarkdown::html_vignette. pdf_document -->
vignette: >
  %\VignetteIndexEntry{pmdplyr: Panel Maneuvers in dplyr}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  warning = FALSE,
  message = FALSE,
  comment = "#>"
)
```

```{r setup}
library(pmdplyr)
```

The `pmdplyr` package is an extension to `dplyr` designed for cleaning and managing panel and hierarchical data. It contains variations on the `dplyr` `mutate` and `join` functions that address common panel data needs, and contains functions for managing and cleaning panel data. 

Unlike other panel data packages, functions in `pmdplyr` are all designed to work even if there is more than one observation per individual per period. This comes in handy if each individual is observed multiple times per period - for example, multiple classes per student per term; or if you have hierarchical data - for example, multiple companies per country.

`pmdplyr` contains a long list of functions for working with panel data, described below.

-----

# pibble

The `pibble` data type is a `tibble` data frame with additional attributes `.i`, which is a set of variables in the `pibble` that identifies individuals, `.t`, which is a variable in the `pibble` that indentifies the time index, and `.d`, which identifies the gap between periods of `.t`. If the gap between time periods doesn't matter and any consecutive time periods should be treated as consecutive, set `.d = 0`.

`pibble` status will be maintained if the `pibble` is modified using functions from `pmdplyr` or `dplyr` (unless you use those functions in a way that drops or renames the `.i` or `.t` variables, in which case `pibble` status will be lost). Other data manipulation functions may remove `pibble` status, requiring it to be re-declared as a `pibble`. 

Most functions in `pmdplyr` will allow you to declare `.i` and `.t` in the function itself. But if a `pibble` is passed in via a `dplyr` verb, there is no need.

## pibble() and as_pibble()

`pibble`s can be declared in two main ways: raw, via `pibble()`:

```{r, eval=FALSE}
pibble(...,
       .i = NULL,
       .t = NULL,
       .d = 1,
       .uniqcheck = FALSE)
```

or by transforming an existing `data.frame`, `list`, or `tbl_df` using `as_pibble()`:

```{r, eval=FALSE}
as_pibble(x,
       .i = NULL,
       .t = NULL,
       .d = 1,
       .uniqcheck = FALSE,
       ...)
```

Both functions work exactly as `tibble::tibble` and `tibble::as_tibble` do, except that they also take the arguments `.i`, `.t`, and `.d`, with `.i` and `.t` accepting either unquoted or quoted variable names. If you'd like your `pibble` checked to see if `.i` and `.t` uniquely identify your observations, set `.uniqcheck = TRUE`. It will do this automatically the first time in each R session you create a `pibble`, but if you'd like it to keep doing it, use `uniqcheck`. 

As a side bonus, you can check if the variables `a, b, c` uniquely identify the observations in data set `d` by running `as_pibble(d, .i = c(a, b, c), .uniqcheck = TRUE)`. No warning? It's uniquely identified!

```{r}
# .d = 1 by default, so in this data,
# a = 1, b = 3 comes one period after a = 1, b = 2.
basic_pibble <- pibble(a = c(1, 1, 1, 2, 2, 2),
                       b = c(1, 2, 3, 2, 3, 3),
                       c = 1:6,
                       .i = a,
                       .t = b)

data(SPrail)
# In SPrail, insert_date does not imply regular gaps between
# time periods, so we set .d = 0
declared_pibble <- as_pibble(SPrail,
                             .i = c(origin, destination),
                             .t = insert_date,
                             .d = 0)
```

## panel_convert()

`pmdplyr` also has the function `panel_convert()` which allows you to convert between different popular R panel data objects, including `pibble`. This can come in handy for creating `pibbles`, or exporting your cleaned `pibble` to use with a package that does panel data *analysis* (which `pmdplyr` does not):

```{r, eval = FALSE}
panel_convert(data,
              to,
              ...)
```

Where `data` is a panel data object, either `pibble`, `tsibble`, `pdata.frame`, or `panel_data`, and `to` is the type of object you'd like returned, which you can refer to by object name, object class, or package name: get a `pibble` with `"pmdplyr"`, `"pibble"`, or `"tbl_pb"`, a `tsibble` with `"tsibble"` or `"tbl_ts"`, a `pdata.frame` with `"plm"` or `"pdata.frame"`, or a `panel_data` with `"panelr"` or `"panel_data"`. `...` sends additional arguments to the functions used to declare those objects.

When using `panel_convert`, be aware that any grouping will be lost, and you must have the relevant package of your `to` option installed (`tsibble`, `plm`, or `panelr`). When your `data` object is a `pdata.frame`, it is recommended to also have `sjlabelled` installed.

All valid objects of the non-`pibble` types can be converted to `pibbles`, but the reverse is not true, since `pibble` does not enforce some strict requirements that other types do:


Feature/Requirement |  `pibble`   |  `tsibble` |  `pdata.frame` |  `panel_data` 
---------------------|-----------|----------------|--------------|------------
ID                    | `.i` | `key`     | `index[1]` | `id` 
Time                  | `.t` | `index`   | `index[2]` | `wave` 
Gap control           | `.d` | `regular` | No              | No  
ID must exist         | No        | No             | Yes             | Yes 
Time must exist       | No        | Yes            | Yes             | Yes[1] 
Only one ID variable[2]| No        | No            | Yes             | Yes 
Unique identification | No        | Yes            | No[3]           | No[3]

[1] `pdata.frame` does not require that time be provided, but if not provided will create it based on original ordering of the data. The `pdata.frame` option to set `index` equal to an integer for a balanced panel and have it figure out the rest by itself is not supported.

[2] Use `id_variable()` (described below) to generate a single ID variable from multiple if one is required.

[3] `pdata.frame` and `panel_data` do not require that ID and time uniquely identify the observations on declaring the data, but functions in these packages may not work correctly without unique identification.

In addition to the above, be aware that the different packages have different requirements on which variable classes can be Time variables. `time_variable()` (described below) can build an integer variable that will work in all packages.

------

# ID and Time Variables

Any panel data dataset is defined by ID variable(s) (`.i`) that indicate the individual person/firm/country/etc. you're talking about, and a time variable (`.t`) that tell you when the observation in question was recorded.

While `pmdplyr` allows multiple ID variables, many panel data packages only allow one. `id_variable()` allows you to turn multiple ID variables into a single variable.

Many panel data packages, including `pmdplyr`, prefer a time variable that is an integer, so that the difference between each period is 1 (or `.d` in `pmdplyr`'s case). The function `time_variable()` can help you generate an integer time variable from a `Date`-class variable, or from one or more variables containing, for example, year and month.

## id_variable()

`id_variable()` syntax follows:

```{r, eval=FALSE}
id_variable(...,
       .method = "number",
       .minwidth = FALSE)
```

where `...` is the set of identity variables that you want to combine into a single one (or, potentially, a single variable you'd like to encode numerically).

`.method` describes the way in which you'd like the variable encoded:

* `.method = number` assigns consecutive numeric codes in the original order they appear in the data.
* `.method = random` assigns numeric codes from `0` to `10*N` (where `N` is the number of codes to be assigned) in a random order, so it will be more difficult to uncover the original identity variables.
* `.method = character` preserves all original information and combines the variables together into a string, adding spacing to ensure uniqueness. Set `.minwidth = TRUE` to remove the spacing, although this may lead to non-uniqueness in some cases.

```{r}
df <- data.frame(country = c("US", "US", "US", "US", 
                             "GBR", "GBR", "GBR", "GBR"),
                 city = c("NYC", "NYC", "Cambridge", "NYC", 
                          "Cambridge", "London", "Manchester", "Manchester")) %>%
  mutate(numeric_ID = id_variable(country, city),
         random_ID = id_variable(country, city, .method = "random"),
         char_ID = id_variable(country, city, .method = "character"))

df
```

## time_variable()

`time_variable()` syntax follows:

```{r, eval = FALSE}
time_variable(...,
              .method = "present",
              .datepos = NA,
              .start = 1,
              .skip = NA,
              .breaks = NA,
              .turnover = NA,
              .turnover_start = NA)
```

Where `...` is the set of variables that you want to combine into a single, `integer`-class time variable. The rest of the options determine how the variable(s) will be read or transformed; the need for each varies depending on the structure of the original data and which `.method` is used.

`.method` can take the values:

* `.method="present"` will assume that, even if each individual may have some missing periods, each period is present in your data *somewhere*, and so simply numbers, in order, all the time periods observed in the data.
* `.method="year"` can be used with a single `Date`/`POSIX`/etc.-type variable (anything that allows `lubridate::date()`) and will extract the year from it. Or, use it with a character or numeric variable and indicate with `.datepos` the character/digit positions that hold the year in YY or YYYY format.  If combined with `.breaks` or `.skip`, will instead set the earliest year in the data to 1 rather than returning the actual year.
* `.method="month"` can be used with a single `Date`/`POSIX`/etc.-type variable (anything that allows `lubridate::date()`). It will give the earliest-observed month in the data set a value of `1`, and will increment from there. Or, use it with a character or numeric variable and indicate with `.datepos` the character/digit positions that hold the year and month in YYMM or YYYYMM format (note that if your variable is in MMYYYY format, for example, you can just give a `.datepos` argument like `c(3:6,1:2)`). Months turn over on the `.start` day of the month, which is by default 1.
* `.method="week"` can be used with a single `Date`/`POSIX`/etc.-type variable (anything that allows `lubridate::date()`). It will give the earliest-observed week in the data set a value of `1`, and will increment from there. Weeks turn over on the `.start` day, which is by default 1 (Monday). Note that this method always starts weeks on the same day of the week, which is different from standard `lubridate` procedure of counting sets of 7 days starting from January 1.
* `.method="day"` can be used with a single `Date`/`POSIX`/etc.-type variable (anything that allows `lubridate::date()`). It will give the earliest-observed day in the data set a value of `1`, and increment from there. Or, use it with a character or numeric variable and indicate with `.datepos` the character/digit positions that hold the year and month in YYMMDD or YYYYMMDD format. To skip certain days of the week, such as weekends, use the `.skip` option.
* `.method="turnover"` can be used when you have more than one variable in variable and they are all numeric nonnegative integers. Set the `.turnover` option to indicate the highest value each variable takes before it starts over, and set `.turnover_start` to indicate what value it takes when it starts over. Cannot be combined with `.skip` or `.breaks`. Doesn't work with any variable for which the turnover values change, i.e. it doesn't play well with days-in-month - if you'd like to do something like year-month-day-hour, I recommend running `.method="day"` once with just the year-month-day variable, and then taking the result and combining *that* with hour in `.method="turnover"`.

```{r}
data(SPrail)

# Since we have a date variable, we can easily create integers that increment for each
# year, or for each month, etc.
# Likely we'd only really need one of these four, depending on our purposes
SPrail <- SPrail %>%
  dplyr::mutate(
    year_time_id = time_variable(insert_date, .method = "year"),
    month_time_id = time_variable(insert_date, .method = "month"),
    week_time_id = time_variable(insert_date, .method = "week"),
    day_time_id = time_variable(insert_date, .method = "day")
  )

# Let's see what we've got
head(SPrail %>% select(insert_date, ends_with("time_id")))

# Perhaps I'd like quarterly data
# (although in this case there are only two months, not much variation there)
SPrail <- SPrail %>%
  dplyr::mutate(quarter_time_id = time_variable(insert_date,
    .method = "month",
    .breaks = c(1, 4, 7, 10)
  ))
# Should line up properly with month
table(SPrail$month_time_id, SPrail$quarter_time_id, dnn = c('Month', 'Quarter'))

# Maybe I'd like Monday to come immediately after Friday!
SPrail <- SPrail %>%
  dplyr::mutate(weekday_time_id = time_variable(insert_date,
    .method = "day",
    .skip = c(6, 7)
  ))

# Perhaps I'm interested in ANY time period in the data and just want to enumerate them in order
SPrail <- SPrail %>%
  dplyr::mutate(any_present_time_id = time_variable(insert_date,
    .method = "present"
  ))

# Note the weekday_time_id NAs - these are weekends! We told it to skip those.
head(SPrail %>% select(insert_date, day_time_id, weekday_time_id, any_present_time_id))

# Maybe instead of being given a nice time variable, I was given it in string form
SPrail <- SPrail %>% dplyr::mutate(time_string = as.character(insert_date))
# As long as the character positions are consistent we can still use it
SPrail <- SPrail %>%
  dplyr::mutate(day_from_string_id = time_variable(time_string,
    .method = "day",
    .datepos = c(3, 4, 6, 7, 9, 10)
  ))
# Results are identical from using the actual Date variable
cor(SPrail$day_time_id, SPrail$day_from_string_id)
```

----- 

# Filling in Data

In a panel data context, missing data can be categorized into two kinds: explicit missing values for certain observations (`NA`s), or observations that are missing entirely - for example if person `1` has observations in period `1` and period `3`, but not period `2`.

You may wish to fill in either of these kinds of missing data using the data you do have.

## panel_fill()

`panel_fill()` will fill in gaps between time periods for individuals. For example, if person `1` has observations in period `1` and period `3`, but not period `2`, then `panel_fill()` will add an observation to the data for person `1` in time period `2`. If there is more than one observation for person `1` in period `1`, then all of them will be copied for period `2`.

```{r, eval = FALSE}
panel_fill(.df,
           .set_NA = FALSE,
           .min = NA,
           .max = NA,
           .backwards = FALSE,
           .group_i = TRUE,
           .flag = NA,
           .i = NULL,
           .t = NULL,
           .d = 1,
           .uniqcheck = FALSE,
           .setpanel = TRUE)
```

`panel_fill()` will give us some newly-created observations, and we need to decide what to fill them in with. By default, it will fill in values using what we see in the most recent non-missing observation. But we can set `.backwards = TRUE` to use the *next* non-missing observation instead, or use `.set_NA` to fill the new observations with missing data.

`.set_NA` is a character vector of variable names that should be set to `NA` for newly-created observations, or set to `TRUE` to set everything except `.i` and `.t` to `NA`. You can also create a new variable indicating which observations are newly-created with `.flag`.

```{r}
# Note the gap between periods 2 and 4 for person 1.
df <- pibble(i = c(1, 1, 1, 2, 2, 2),
             t = c(2, 4, 5, 1, 2, 3),
             x = 1:6,
             y = 7:12,
             .i = i,
             .t = t)

panel_fill(df, .set_NA = "y", .flag = "new_obs")
panel_fill(df, .set_NA = "y", .backwards = TRUE)$x
```

By default, `panel_fill()` will only fill in gaps between existing observations. However, commonly we might want to create new observations outside of the existing range, perhaps to create a fully balanced panel for ourselves. `.min` and `.max` will ensure that each individual has observations at least as far back as `.min`, and at least as far out as `.max`. Set `.min = min(t)` and `.max = max(t)` (where `t` is your time variable) to ensure a fully balanced panel.

Data for the outside-the-observed-range values will be taken from the closest observed value.

```{r}
panel_fill(df, .min = min(df$t), .max = max(df$t))
```

The rest of the options include `.group_i` (by default, if `.i` can be found, data will be filled within-individual. Set `.group_i = FALSE` to ignore this), and standard arguments related to declaring the panel structure of the data (`.i`, `.t`, `.d`, `.uniqcheck`, see the "pibble" section above). `.setpanel` ensures that if you declare the panel structure in the `panel_fill()` function, it will be maintained in the object you get back.

## panel_locf()

`panel_locf()` ("last observation carried forward") will fill in explicit `NA` values using recently available data. It is very similar to `zoo::na.locf()` except that it respects panel structure and is more flexible.

```{r, eval = FALSE}
panel_locf(.var,
           .df = get(".", envir = parent.frame()),
           .fill = NA,
           .backwards = FALSE,
           .resolve = "error",
           .group_i = TRUE,
           .i = NULL,
           .t = NULL,
           .d = 1,
           .uniqcheck = FALSE)
```

where `.var` is the variable to be filled in, and `.df` is the data set that variable lives in. If the data set is being passed in via `%>%`, then `.df` will automatically pick it up and you don't need to specify it.

```{r}
df <- pibble(i = c(1, 1, 1, 2, 2, 2),
             t = c(1, 2, 3, 2, 3, 4),
             x = c(1, NA, 3, NA, -3, 4),
             .i = i,
             .t = t)

# Notice that the fourth observation doesn't get filled in
# because it's the first observation for person 2, so nothing to fill in from
df %>% 
  mutate(x_filled = panel_locf(x))
```

You have a fair amount of control over how filling-in works. By default, data will be filled in using the most recent previous observation. But `.backwards = TRUE` will use the *next upcoming* observation instead. Also, by default, only `NA` values will be overwritten. But `.fill` will allow you to specify a vector of values (perhaps including `NA`) to be overwritten. This can be handy if you're working with data that uses missingness indicators other than `NA`.

```{r}
df %>% mutate(x_filled = panel_locf(x, .backwards = TRUE),
              x_no_neg3 = panel_locf(x, .backwards = TRUE, .fill = c(NA, -3)))
```

`panel_locf()` will work even if `.i` and `.t` don't uniquely identify the observations. However, this presents a problem! If there are *different values* of `.var` for a given combination of `.i` and `.t`, then which value do we choose to use for the purpose of filling in other observations? `.resolve` makes this choice. By default, there will be an "error" if values of `.var` are inconsistent within `.i` and `.t`. Or, set `.resolve` to a summary function like `.resolve = mean` or `.resolve = function(x) mean(x, na.rm = TRUE)` to resolve inconsistencies before filling in. If you have some `.i`/`.t` combinations with both missing and non-missing values, the missing values will be filled in using the same function.

```{r}
inconsistent_df <- pibble(i = c(1, 1, 1, 2, 2, 2),
                          t = c(1, 1, 2, 1, 2, 3),
                          x = c(1, 2, NA, 1, 2, 3),
                          .i = i,
                          .t = t)

inconsistent_df %>% mutate(x_filled = 
                             panel_locf(x, .resolve = mean))

```

The rest of the options include `.group_i` (by default, if `.i` can be found, data will be filled within-individual. Set `.group_i = FALSE` to ignore this), and standard arguments related to declaring the panel structure of the data (`.i`, `.t`, `.d`, `.uniqcheck`, see the "pibble" section above). `.setpanel` ensures that if you declare the panel structure in the `panel_fill()` function, it will be maintained in the object you get back.

----- 

# Panel Consistency

In panel data, and especially hierarchical data, there are some variables that should be *fixed* within values of other variables. And if they're not, you have a problem!

For example, consider the data set

```{r}
df <- data.frame(continent = c("Asia", "Europe", "Europe", "S America", "S America"),
           country = c("France", "France", "France", "Brazil", "Brazil"),
           year = c(2000, 2001, 2002, 2000, 2001))

df
```

The variable `continent` should never change within values of `country` - a country can't change the continent it's on! The fact that France changes continents from year to year in this data should be regarded as very fishy. It will be handy to spot these sorts of potential errors in your data set, and fix them if you think you know how.

## fixed_check()

`fixed_check()` will look in your data `.df` for inconsistencies in the value of some variables `.var` within values of other variables `.within`.

```{r, eval = FALSE}
fixed_check(.df,
            .var = NULL,
            .within = NULL)
```

You should pick variables for `.var` that are supposed to be constant within combinations of `.within`.

If your data has problems and is inconsistent, `fixed_check()` will retun a list of data sets, one for each `.var` variable, containing the subset of the data that gives you problems. For our `df` with the France problem, that's all of the France observations!

```{r}
fixed_check(df, .var = continent, .within = country)$continent
```

If your data is fine, and all `.var` variables are indeed constant within combinations of `.within`, then `fixed_check()` will return TRUE.

```{r}
consistent_df <- data.frame(state = c(1, 1, 1, 2, 2, 2),
                            year = c(2000, 2001, 2001, 2000, 2000, 2001),
                            treatment = c(F, T, T, T, T, F),
                            outcome = c(4.4, 3.2, 3.4, 5.5, 5.6, 8))

# Since this policy treatment is administered on the state level,
# everyone in the same state/year should get the same treatment.
# And they do!
fixed_check(consistent_df, .var = treatment, .within = c(state, year))
```
                            
Some handy `fixed_check()` tips:

1. `fixed_check()` returns either a `logical` or a `list` depending on the outcome. If you want to just get `FALSE` instead of a list of data sets, do `is.logical(fixed_check())` instead of `fixed_check()`.
2. If you do have problems and want a consistent data set, you can fix things by hand as you like, or you can use either use `fixed_force()` (see below) or combine the list of data sets returned by `fixed_force()` with `dplyr::anti_join()`. 

```{r}
anti_join(df,
          fixed_check(df, .var = continent, .within = country)$continent)
```

## fixed_force()

`fixed_force()` will take a data set `.df`, find any inconsistencies in the variables `.var` within combinations of the variables `.within`, and will "fix" those inconsistencies, using the function `.resolve` to select the correct values. It will flag any changed values with a new variable named `.flag`.

```{r, eval = FALSE}
fixed_force(..df,
            .var = NULL,
            .within = NULL,
            .resolve = mode_order,
            .flag = NA)
```

The default resolution function is `mode_order()` (see the Additional Calculations section), which calculates the mode, selecting the first-ordered value in the data if there are ties. The mode seems most relevant here, since the most likely (and responsible) use for `fixed_force()` is when you have data that is mostly correct but just has a few odd values that are likely just miscodes. `mode_order()` also is not just limited to numeric variables.

Continuing with our France-in-Asia data set,

```{r}
fixed_force(df, .var = continent, .within = country, .flag = "altered")
```

-----

# Joins

`pmdplyr` offers a set of wrappers for the `dplyr::join` functions.

## inexact_join

The set of `inexact_join` functions maps directly onto the set of `dplyr::join` functions: `inexact_inner_join, inexact_left_join, inexact_right_join, inexact_full_join, inexact_semi_join, inexact_nest_join`, and `inexact_anti_join`. 

Here we will focus specifically on `inexact_left_join`; for the differences between the functions see the descriptions of the original join functions at `help(join, package = "dplyr")`.

`join` functions take two data sets and join them based on matching values of a set of shared variables.

```{r}
left_df <- data.frame(i = c(1, 1, 1, 2, 2, 2),
                      t = c(1, 2, 3, 1, 2, 3),
                      v1 = 1:6)
right_df <- data.frame(i = c(1, 1, 1, 2, 2, 2),
                       t = c(0, 2, 4, 0, 2, 4),
                       v2 = 7:12)

# It automatically detects that i and t are the shared variables
# and finds two combinations of those in left_df that are also
# in right_df: i = 1, t = 2, and i = 2, t = 2. So it brings the 
# v2 values it can match up in to the joined data.
# Other observations don't find a match
left_join(left_df, right_df)
```

However, it is common (especially in a panel data context) to want to join two data frames where one of the variables does not line up exactly. For example, maybe we want those `t = 1` values in `left_df` to pick up the `t = 0` values in `right_df`. 

We can do this, in a few different ways with an `inexact_join`:

```{r, eval = FALSE}
inexact_left_join(x, y,
                  by = NULL,
                  copy = FALSE,
                  suffix = c(".x", ".y"),
                  ...,
                  var = NULL,
                  jvar = NULL,
                  method,
                  exact = TRUE)
```

The first arguments: `x, y, by, copy, suffix, ...`, are standard arguments to be passed to `left_join`. `x` and `y` are our left-hand and right-hand data sets, respectively. See `help(left_join, package = "dplyr")` for the rest.

We've added on here `var, jvar, method`, and `exact`.

`var` is the variable in the left-hand data set that you would like to match inexactly on, and `jvar` is the variable(s) in the right-hand data set that you would like to match inexactly on. It's important that the names of these variables aren't shared, because the resulting data set will show how `var` and `jvar` line up. So let's prepare our data by renaming `t` in `right_df` to something else so it's not `t` in both data sets.

```{r}
right_df <- right_df %>%
  rename(t_right = t)
```

`method` determines *how* `var` and `jvar` will be matched up.

* `method = "last"` matches `var` to the closest value of `jvar` that is *lower*, so those `t = 1` observations will get matched to `t_right = 0`, and `t = 3` will get matched to `t_right = 2` (meaning that `t_right = 2` will get matched to both `t = 2` and `t = 3`):

```{r}
inexact_left_join(left_df, 
                  right_df,
                  var = t, jvar = t_right,
                  method = "last")
```

* `method = "next"` matches `var` to the closest value of `jvar` that is *higher*, so now `t = 1` will get matched to `t_right = 2`, and `t = 3` will get matched to `t_right = 4`:

```{r}
inexact_left_join(left_df, 
                  right_df,
                  var = t, jvar = t_right,
                  method = "next")
```

* `method = "closest"` will match `var` to the closest value of `jvar` in either direction. If there's a tie, it will pick the lower value of `jvar`. So now `t = 1` will pick `t_right = 0` (out of a tie between `0` and `2`), and `t = 3` will match to `t = 2`:

```{r}
inexact_left_join(left_df, 
                  right_df,
                  var = t, jvar = t_right,
                  method = "closest")
```

* Finally, `method = "between"` is for matching `var` to a set of two `jvar`s that define the beginning and end of a *range*. Make sure that the ranges are non-overlapping within the joining variables, or else you will get strange results (specifically, it should join to the earliest-starting range). So now, given the way we define `t_bottom` and `t_top` below, `t = 1` should go in the range `t_bottom = 0, t_top = 2`, and `t = 2` and `t = 3` should both go in the range `t_bottom = 2, t_top = 4`.

```{r}
right_df <- right_df %>%
  rename(t_bottom = t_right) %>%
  mutate(t_top = t_bottom + 2)

inexact_left_join(left_df, 
                  right_df,
                  var = t, jvar = c(t_bottom, t_top),
                  method = "between")
```

So that leaves us with `exact`. `exact` determines whether or not an exact match is an acceptable match, and interprets `"last"` as "this value or earlier" and `"next"` as "this value or later". Generally, for joining purposes, you'll want this to be `TRUE`. But perhaps you don't! Maybe you want "earlier" or "later" only to get something like "the most recent previous value" for `method = "last"`. In that case, set this to `FALSE`.

In the case of `method = "between"`, it's especially important to keep track of `exact` because it's common for one range to start at the exact endpoint of another. If the end of one range is the exact start of another, `exact = c(TRUE,FALSE)` or `exact = c(FALSE,TRUE)` is recommended to avoid overlaps. Defaults to `exact = c(TRUE,FALSE)`.

## safe_join()

When joining two data sets `x` and `y` on a set of shared variables `by`, there are four ways in which they can be matched: one-to-many (`by` uniquely identifies rows in `x` but not `y`, so each observation in `x` will be matched to several in `y`), many-to-one (`by` uniquely identifies rows in `y` but not `x`, so each observation in `y` will be matched to several in `x`), one-to-one (`by` uniquely identifies rows in both `x` and `y`, so each observation in `x` will be matched to exactly one in `y`), and many-to-many (`by` does not uniquely identify rows in either `x` or `y`).

Unfortunately, when you perform a `join` or `inexact_join`, it doesn't tell you which of those you've just done! This can be especially problematic if you've accidentally done a many-to-many join, since many-to-many join often leads to unexpected results.

`safe_join()` is a wrapper for all `join` and `inexact_join` functions which tells you whether you are, in fact, doing the join you expect to be doing, and returns an error if you're not.

```{r, eval = FALSE}
safe_join(x, y,
          expect = NULL,
          join = NULL,
          ...)
```

`x`, `y`, and `...` are the standard `join`/`inexact_join` arguments that you would normally use. See `help(join, package = "dplyr")` or the `inexact_join` section above to see what arguments might go in `...` to pass through to those functions, such as `suffix` or `var`.

`expect` is a character variable where you specify the type of join you *think* you're about to do. You can specify this either as one-to-many / many-to-one / one-to-one directly, or you can specify which of the two data sets (`x` or `y`) you think should be uniquely identified by the joining variables.

* `expect = "1:1" or `expect = c("x", "y")` or `expect = "xy"` indicates that you anticipate to join one-to-one. 
* `expect = "m:1" or `expect = "y"` indicates that you expect to join many-to-one.
* `expect = "1:m" or "expect = "x"` indicates that you expect to join one-to-many.
* `expect = "no m:m"` indicates that you don't care whether you're one-to-one, one-to-many, or many-to-one, as long as you're not many-to-many.
* There is no `expect` option that allows you to run a many-to-many join.

`safe_join` will return an error if your data do not match your `expect` selection.

If your data *does* match your `expect` option, then it will look to your `join`. `join` is the function for the `join` or `inexact_join` you'd like to run, for example `join = inexact_left_join`.

If run without a `join` specified, `safe_join()` will return `TRUE` if you're good to go. If run with a `join` specified, then instead `safe_join()` will pass your data on to the function and actually run the join for you.

There is little reason to run any `join` or `inexact_join` without going through `safe_join()`. It will help you avoid some nasty surprises!

```{r}
# left is panel data and i does not uniquely identify observations
left <- data.frame(
  i = c(1, 1, 2, 2),
  t = c(1, 2, 1, 2),
  a = 1:4
)
# right is individual-level data uniquely identified by i
right <- data.frame(
  i = c(1, 2),
  b = 1:2
)

# I think that I can do a one-to-one merge on i
# Forgetting that left is identified by i and t together
# So, this produces an error
try(
  safe_join(left, right, expect = "1:1", join = left_join)
)

# If I realize I'm doing a many-to-one merge, that is correct,
# so safe_join will return TRUE if we don't specify a join
# or perform the join for us if we do
safe_join(left, right, expect = "m:1")
safe_join(left, right, expect = "m:1", join = left_join)
```

-----

# Mutate Variations

`pmdplyr` adds several new versions of `dplyr::mutate()` that help with some common panel-data manipulation needs.

## mutate_subset()

`mutate_subset()` is a function that performs a `dplyr::summarize()` command on a subset (`.filter`) of your data, and then takes the result and applies it to all observations in your data (or all observations within group, if grouped).

The most common use of this is to *partially widen* your data. Panel data can be stored in "wide" format, where there is one row per individual and, for each variable, one column per time period, or the more common (and assumed in `pmdplyr`) "long" format where there is one (or possibly more than one) row per individual/time period.

The benefit of wide data is that it makes it very easy to compare variables across wide stretches of time. How much has asset `1` increased in value from the beginning of the sample? Easy in wide data, a little trickier in long (although it could be done with a `tlag()`, see tlag() section). 

If you only have a few such comparisons to make, `mutate_subset()` lets you make them without fully widening the data. Just make a "value at the beginning of the sample" variable, if that's all you need, without having to bother fully widening.

Another common use is to make specific comparisons within groups. If I want to know how your earnings compare to the average earnings in your state, I can just do a `within_i()` calculation (see Additional Calculations section). But what if I want to know how your earnings compare to the average earnings *of college graduates* in your state? That's harder. But `mutate_subset()` makes it easy.

```{r, eval = FALSE}
mutate_subset(.df,
              ...,
              .filter,
              .group_i = TRUE,
              .i = NULL,
              .t = NULL,
              .d = NA,
              .uniqcheck = FALSE,
              .setpanel = TRUE)
```

where `.df` is the data set being mutated and `...` is a set of name-value pairs of expressions in the style of `dplyr::mutate`. Note that, since the idea here is to get a summary measure from a filtered group, expressions should be written such that they would be valid arguments in `dplyr::summarize()`.

`.filter` is a logical condition that describes the observations that you want to perform the `...` calculations on.

Let's perform the analysis we described above, comparing an individual's earnings to the average earnings of college graduates in their state:

```{r}
df <- pibble(state = c("CA", "CA", "CA", "NV", "NV", "NV"),
             college = c(TRUE, TRUE, FALSE, TRUE, FALSE, FALSE),
             earn = c(1, 2, 3, 2, 3, 2),
             .i = state)

df %>%
  # Calculate average earnings of college grads
  mutate_subset(college_earnings = mean(earn), .filter = college == TRUE) %>%
  # And compare to our own earnings
  mutate(earnings_vs_college = earn - college_earnings)
```

The rest of the options include `.group_i` (by default, if `.i` can be found, analysis will be performed within-individual. Set `.group_i = FALSE` to ignore this), and standard arguments related to declaring the panel structure of the data (`.i`, `.t`, `.d`, `.uniqcheck`, see the "pibble" section above). The `.d = NA` will become `.d = 1` if either `.i` or `.t` are declared. `.setpanel` ensures that if you declare the panel structure in the `panel_fill()` function, it will be maintained in the object you get back.

## mutate_cascade()

`mutate_cascade()` performs `dplyr::mutate()` on a data set one time period at a time, in order, allowing the mutate from each time period to finish before moving on to the next one. The changes "cascade down" through time. This can be handy if your `mutate()` command makes reference to earlier time periods using (usually) `tlag()` (see below) and you want changes in one period to be passed down to the next. 

In effect, you can think of `mutate_cascade()` as behaving much like `cumsum()`, `cumprod()`, `cummax()` or `cummin()`, except that it (1) respects the panel structure of the data, (2) works when you have multiple observations per `.i`/`.t`, (3) is much more flexible, and (4) is much slower.

As of this writing `mutate_cascade()` is pretty darn slow (after all, if you have T time periods, you're running T separate `mutate` commands in a loop!), so be careful in using it.

```{r, eval = FALSE}
mutate_cascade(.df,
               ...,
               .skip = TRUE,
               .backwards = FALSE,
               .group_i = TRUE,
               .i = NULL,
               .t = NULL,
               .d = NA,
               .uniqcheck = FALSE,
               .setpanel = TRUE)
```

where `.df` is the data set being mutated, and `...` is the list of expressions to be passed to `dplyr::mutate()`.

`.skip` instructs `mutate_cascade()` to skip over the first time period (or last time period if `backwards = TRUE`). This should usually be set to `TRUE`, since most usages of `mutate_cascade()` involve a `tlag()`, and the `tlag()` of something in the first time period is usually `NA`. Then, you've filled in that first-period `NA` - now the `tlag()` in period 2 is NA as well, and it will cascade down to make your whole data set `NA`.

`.backwards`, unsurprisingly, tells `mutate_cascade()` to start with the last time period and work backwards.

Let's do a very simple example and use `mutate_cascade()` to build a present discounted value. We have an asset with a `payout` each period, and we have a discount factor `.95`. We can build a present discounted value `PDV` by taking the `PDV` in the next period, multiplying it by `.95`, and adding on the current `payout`. But we need to calculate `PDV` one period at a time, so that we can use each period's calculation to calculate the previous one.

```{r}
df <- pibble(t = c(1, 2, 3, 4, 5),
             payout = c(3, 4, 2, 2, 4),
             .t = t) %>%
  mutate(PDV = payout) %>%
  mutate_cascade(PDV = payout + .95*tlag(PDV, .n = -1), .backwards = TRUE)
```

As expected, the `PDV` in period `5` is just the payout: `4`. In period `4` it's `2 + .95*4 = 5.8`. Then in period `3` it's `2 + .95*5.8 = 7.51`, and so on.

The rest of the options include `.group_i` (by default, if `.i` can be found, analysis will be performed within-individual. Set `.group_i = FALSE` to ignore this), and standard arguments related to declaring the panel structure of the data (`.i`, `.t`, `.d`, `.uniqcheck`, see the "pibble" section above). The `.d = NA` will become `.d = 1` if either `.i` or `.t` are declared. `.setpanel` ensures that if you declare the panel structure in the `panel_fill()` function, it will be maintained in the object you get back.

-----

# tlag()

`tlag()` is a function that lags a variable in time. It respects the panel structure of the data, works with multiple observations per combination of `.i`/`.t`, and, unlike `plm::lag()`, doesn't run into masking problems by sharing a name with `dplyr::lag()`. Do remember that `dplyr::lag()` does not lag data in time, it lags data in the order of the data set.

```{r, eval = FALSE}
tlag(.var,
     .df = get(".", envir = parent.frame()),
     .n = 1,
     .default = NA,
     .quick = FALSE,
     .resolve = "error",
     .group_i = TRUE,
     .i = NULL,
     .t = NULL,
     .d = NA,
     .uniqcheck = FALSE)
```

where `.var` is the variable being lagged, , and `.df` is the data set that variable lives in. If the data set is being passed in via `%>%`, then `.df` will automatically pick it up and you don't need to specify it.

`.n` is the number of periods to lag. Negative values of `.n` imply a lead instead of a lag (as in the example in `mutate_cascade()` in the Mutate Variations section). There's not a separate `tlead()` function.

`.default` is the value to use if a lag does not exist. By default, this is `NA`. So if you have data in periods `1` and `3` but not `2`, then the `tlag` in the third period will produce `NA`.

`.quick` is a setting you can use if your data is very nicely structured, with rows uniquely identified by `.i`/`.t` and there are either no gaps between time periods or `.d = 0`. `tlag()` will run more quickly with `.quick = TRUE`, but will produce incorrect results if these conditions are not met.

```{r}
df <- pibble(i = c(1, 1, 1, 2, 2, 2),
             t = c(1, 2, 3, 1, 2, 3),
             x = 1:6,
             .i = i,
             .t = t) %>%
  # A lag and a lead, filling in the lead with 0 instead of NA
  mutate(x_lag = tlag(x),
         x_lead = tlag(x, .n = -1, .default = 0),
         # Our data satisfies the .quick conditions so we can 
         # do that for a little extra speed
         x_quicklag = tlag(x, .quick = TRUE))

df
```

If `.var` is not constant within combinations of `.i` and `.t` we have a problem! Which value do we choose to use for the purpose of filling in other observations? `.resolve` makes this choice. By default, there will be an "error" if values of `.var` are inconsistent within `.i` and `.t`. Or, set `.resolve` to a summary function like `.resolve = mean` or `.resolve = function(x) mean(x, na.rm = TRUE)` to resolve inconsistencies before filling in.

```{r}
df <- pibble(i = c(1, 1, 1, 2, 2, 2),
             t = c(1, 1, 2, 1, 1, 2),
             x = 1:6,
             .i = i,
             .t = t) %>%
  mutate(x_lag = tlag(x, .resolve = mean))

df
```

The rest of the options include `.group_i` (by default, if `.i` can be found, lags will be performed within-individual. Set `.group_i = FALSE` to ignore this), and standard arguments related to declaring the panel structure of the data (`.i`, `.t`, `.d`, `.uniqcheck`, see the "pibble" section above). The `.d = NA` will become `.d = 1` if either `.i` or `.t` are declared. `.setpanel` ensures that if you declare the panel structure in the `panel_fill()` function, it will be maintained in the object you get back.

-----

# Additional Calculations

`pmdplyr` contains several additional functions that produce calculations of interest.

## between_i()

`between_i()` performs the *between transformation*. In particular, it isolates the variation between `.i` groups in a variable `.var`, throwing out all variation within `.i` groups. The result is identical within combinations of `.i`.

The specific calculation that is performed is

$$between.i(x) = \bar{x}_i - \bar{x}$$

where $\bar{x}_i$ is the mean of `x` within the `.i` groups, and $\bar{x}$ is the grand mean of `x` over all observations. 


Be aware that this is different from `plm::between()`, which returns $\bar{x}_i$ and does not subtract out $\bar{x}$.

The syntax for `between_i` is:

```{r, eval = FALSE}
between_i(.var,
          .df = get(".", envir = parent.frame()),
          .fcn = function(x) mean(x, na.rm = TRUE),
          .i = NULL,
          .t = NULL,
          uniqcheck = FALSE)
```

Where `.var` is the variable on which the transformation is performed, and `.df` is the data set. If the data set is being passed in via `%>%`, then `.df` will automatically pick it up and you don't need to specify it. `.fcn` is the function applied to calculate the group and grand values, i.e. $.fcn(x) = \bar{x}$. The standard definition of the between transformation is for this to be the mean, but it has been left flexible.

The rest of the options include standard arguments related to declaring the panel structure of the data (`.i`, `.t`, `.uniqcheck`, see the "pibble" section above). `.d` is omitted because it is irrelevant to the calculation.

An example of the between transformation follows:

```{r}
df <- pibble(i = c(1, 1, 2, 2), 
             x = 1:4, 
             .i = i) %>%
  mutate(between_x = between_i(x))

# Notice that the grand mean is...
mean(df$x)
# And the mean within groups is...
df %>%
  group_by(i) %>%
  summarize(x = mean(x))

# So the between calculation should be 
# 1.5 - 2.5 = -1 and 3.5 - 2.5 = 1 for the different groups:
df$between_x
```

## within_i()

`within_i()` performs the *within transformation*. In particular, it isolates the variation within `.i` groups in a variable `.var`, throwing out all variation between `.i` groups. The result averages out to `0` within combinations of `.i`.

The specific calculation that is performed is

$$within.i(x) = x_i - \bar{x}_i$$

where $\bar{x}_i$ is the mean of `x` within the `.i` groups. 

The syntax for `within_i` is:

```{r, eval = FALSE}
within_i(.var,
          .df = get(".", envir = parent.frame()),
          .fcn = function(x) mean(x, na.rm = TRUE),
          .i = NULL,
          .t = NULL,
          uniqcheck = FALSE)
```

Where `.var` is the variable on which the transformation is performed, and `.df` is the data set. If the data set is being passed in via `%>%`, then `.df` will automatically pick it up and you don't need to specify it. `.fcn` is the function applied to calculate the group values, i.e. $.fcn(x) = \bar{x}$. The standard definition of the within transformation is for this to be the mean, but it has been left flexible.

The rest of the options include standard arguments related to declaring the panel structure of the data (`.i`, `.t`, `.uniqcheck`, see the "pibble" section above). `.d` is omitted because it is irrelevant to the calculation.

An example of the between transformation follows:

```{r}
df <- pibble(i = c(1, 1, 2, 2), 
             x = 1:4, 
             .i = i) %>%
  mutate(within_x = within_i(x))

# Notice that the mean within groups is...
df %>%
  group_by(i) %>%
  summarize(x = mean(x))

# So the between calculation should be 
# 1 - 1.5 = -.5 and 2 - 1.5 = .5 for individual 1
# and 3 - 3.5 = -.5 and 4 - 3.5 = .5 individual 2:
df$within_x
```

## mode_order()

R does not have a base function for calculating the mode of a vector. But `fixed_force()` wanted one, and so here we are. This function has been exported for general use in case it comes in handy elsewhere.

In particular, `mode_order()` calculates the mode of a vector and, if there are ties between two different values, selects the one that comes earlier in the original vector order.

```{r}
# 2 appears twice while everything else appears once; 2 is the mode.
x <- c(1, 2, 2, NA, 5, 3, 4)
mode_order(x)

# 1 or 2 could be the mode.
# Ties are broken by order in the vector.
x <- c(2, 2, 1, 1)
mode_order(x)
```
